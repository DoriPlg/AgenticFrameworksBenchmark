services:
  gaia-agent:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: gaia-agent
    environment:
      # Pass through environment variables for API keys
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY:-}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY:-}
      - LANGFUSE_HOST=${LANGFUSE_HOST:-https://cloud.langfuse.com}
      # HuggingFace token for GAIA dataset access
      - HF_TOKEN=${HF_TOKEN}
      # Test configuration variables
      - FRAMEWORK=${FRAMEWORK:-crewai}
      - MODEL_IDX=${MODEL_IDX:-0}
      - NUM_QUESTIONS=${NUM_QUESTIONS:-5}
      - START_IDX=${START_IDX:-0}
      - TEST_MODE=${TEST_MODE:-single}
      - TEMPERATURE=${TEMPERATURE:-0.0}
      - OUTPUT_DIR=${OUTPUT_DIR:-/app/output}
      # Suppress Pydantic serialization warnings (non-critical)
      - PYTHONWARNINGS=ignore::UserWarning:pydantic
      - PYTHONWARNINGS=ignore::RuntimeWarning
      # Add any other environment variables your llmforall.py needs
    extra_hosts:
      # Allow container to access host services via host.docker.internal
      - "host.docker.internal:host-gateway"
    volumes:
      # Optional: Mount output directory for results only
      - ./output:/app/output
      # CrewAI and HF cache in Docker volumes (isolated)
      - gaia-storage:/app/crewai_storage
      - gaia-storage:/app/hf_cache
      # Note: All data is isolated in container volumes
    networks:
      - gaia-network
    # Resource limits for security
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '0.5'
          memory: 512M
    # Prevent container from running as root
    user: "1000:1000"
    # Security options
    security_opt:
      - no-new-privileges:true
    # Tmpfs for temporary files
    tmpfs:
      - /tmp
      - /app/tmp

volumes:
  gaia-storage:
    driver: local

networks:
  gaia-network:
    driver: bridge
